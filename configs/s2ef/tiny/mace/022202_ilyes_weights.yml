trainer: eval_mace_chkpts_trainer

logger: wandb

task:
  dataset: oc22_lmdb  # for training on total energies.
  primary_metric: forces_mae
  train_on_free_atoms: True
  eval_on_free_atoms: True
  # OCP MACE models have "z2idx", which is missing in official MACE models.
  # Official MACE models have "atomic_numbers", "r_max", "num_interactions",
  # which are missing in OCP MACE models. Hence we set strict_load = False.
  strict_load: False

dataset:
  - src: /checkpoint/bmwood/oc20_small/lmdbs/random1215212/v1_sid_str/train/ads_energy/
    total_energy: True
    oc20_ref: /checkpoint/janlan/ocp/other_data/final_ref_energies_02_07_2021.pkl
    # Switch off normalize_labels since scale_shift in the MACE model does the same thing.
    normalize_labels: False
    # target_mean: -0.7554450631141663
    # target_std: 2.887317180633545
    # grad_target_mean: 0.0
    # grad_target_std: 2.887317180633545
    # ase_nbrlist: True
    # ase_cutoff: 5.0
  - src: /checkpoint/bmwood/oc20_small/lmdbs/random1215212/v1_sid_str/val/ads_energy/
    total_energy: True
    oc20_ref: /checkpoint/janlan/ocp/other_data/final_ref_energies_02_07_2021.pkl
    # ase_nbrlist: True
    # ase_cutoff: 5.0
  # - src: /checkpoint/bmwood/oc20_small/lmdbs/random1215212/test/ads_energy

model:
  name: scaleshift_mace
  r_max: 5.0
  avg_num_neighbors: 15.3386
  num_bessel: 8
  num_polynomial_cutoff: 5
  max_ell: 3
  num_interactions: 2
  num_elements: 5
  hidden_irreps: 128x0e + 128x1o
  MLP_irreps: 16x0e
  correlation: 3
  # atomic_energies: ref energy per atomic number.
  atomic_energies: oc20tiny
  # torch.div(energies, natoms).mean()
  atomic_inter_shift: -7.1012316731649495e-15
  # torch.sqrt(torch.mean(torch.square(forces)))
  atomic_inter_scale: 0.23462538421154022
  # OCP flags.
  regress_forces: True
  otf_graph: True
  use_pbc: True

optim:
  batch_size: 64
  eval_batch_size: 64
  load_balancing: atoms
  eval_every: 1000
  num_workers: 2
  # default from (https://github.com/ACEsuit/mace/blob/main/mace/tools/arg_parser.py#L234)
  lr_initial: 1.e-2
  optimizer: AdamW
  optimizer_params: {"amsgrad": True}
  scheduler: ReduceLROnPlateau
  mode: min
  factor: 0.8
  # In the MACE code, the RLROP scheduler is run every epoch even if validation loss is stale.
  # https://github.com/ACEsuit/mace/blob/5470b632d839358faed4e9c97f67fece1b558962/mace/tools/train.py#L149
  # In the OCP code, we step the scheduler only when validation loss is recomputed.
  # https://github.com/ACEsuit/mace-ocp/blob/main/ocpmodels/trainers/forces_trainer.py#L412
  # And so we should probably keep a lower patience. Using default from OCP for now.
  patience: 3
  max_epochs: 1000
  # TODO(@abhshkdz): implement the swa scheduler, i.e. switch to high energy coefficient and low force coefficient after loss plateaus with initial coefficients.
  force_coefficient: 1000
  energy_coefficient: 37
  # default from https://github.com/ACEsuit/mace/blob/main/mace/tools/arg_parser.py#L285
  ema_decay: 0.99
  # default from https://github.com/ACEsuit/mace/blob/main/mace/tools/arg_parser.py#L321
  clip_grad_norm: 10
  # default from https://github.com/ACEsuit/mace/blob/main/mace/tools/arg_parser.py#L240.
  weight_decay: 5.e-7

slurm:
  constraint: "volta32gb"
